
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>propensity_matching package &#8212; propensity_matching  documentation</title>
    <link rel="stylesheet" href="_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <script type="text/javascript" src="_static/documentation_options.js"></script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="nav-item nav-item-0"><a href="propensity_matching_docs.html">propensity_matching  documentation</a> &#187;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="propensity-matching-package">
<h1>propensity_matching package<a class="headerlink" href="#propensity-matching-package" title="Permalink to this headline">¶</a></h1>
<div class="section" id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="module-propensity_matching.config">
<span id="propensity-matching-config-module"></span><h2>propensity_matching.config module<a class="headerlink" href="#module-propensity_matching.config" title="Permalink to this headline">¶</a></h2>
<p>Constants for propensity_matching library.</p>
</div>
<div class="section" id="module-propensity_matching.estimator">
<span id="propensity-matching-estimator-module"></span><h2>propensity_matching.estimator module<a class="headerlink" href="#module-propensity_matching.estimator" title="Permalink to this headline">¶</a></h2>
<p>module holding PropensityEstimator (ml.Estimator).</p>
<dl class="class">
<dt id="propensity_matching.estimator.PropensityEstimator">
<em class="property">class </em><code class="descclassname">propensity_matching.estimator.</code><code class="descname">PropensityEstimator</code><span class="sig-paren">(</span><em>fit_data_prep_args: Union[dict</em>, <em>NoneType] = None</em>, <em>probability_estimator: Union[pyspark.ml.base.Estimator</em>, <em>NoneType] = None</em>, <em>response_col: str = 'response'</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/propensity_matching/estimator.html#PropensityEstimator"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#propensity_matching.estimator.PropensityEstimator" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.6)"><code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></a></p>
<p>ml.Estimator to fit and return a PropensityModel.</p>
<dl class="docutils">
<dt>fit_data_prep_args <span class="classifier-delimiter">:</span> <span class="classifier">dict</span></dt>
<dd>args for class balance and test/train split during fitting</dd>
<dt>probability_estimator <span class="classifier-delimiter">:</span> <span class="classifier">pyspark.ml.Model = LogisticRegression</span></dt>
<dd>currently only supported probability estimator</dd>
<dt>response_col <span class="classifier-delimiter">:</span> <span class="classifier">str = ‘response’</span></dt>
<dd>column containing response variable</dd>
<dt>train_set <span class="classifier-delimiter">:</span> <span class="classifier">pyspark.sql.DataFrame</span></dt>
<dd>training set used by probability estimator.
created by _split_test_train</dd>
<dt>test_set <span class="classifier-delimiter">:</span> <span class="classifier">pyspark.sql.DataFrame</span></dt>
<dd>test set used by probability estimator.
created by _split_test_train</dd>
<dt>rebalanced_df <span class="classifier-delimiter">:</span> <span class="classifier">pyspark.sql.DataFrame</span></dt>
<dd>dataframe with class balance given in fit_data_prep_args</dd>
</dl>
<p>default_probability_estimator_args
default_fit_data_prep_args</p>
<dl class="method">
<dt>
<code class="descname">__init__(</code></dt>
<dd><blockquote>
<div>pred_cols: List[str],
fit_data_prep_args: dict = default_fit_data_prep_args,
probability_estimator_args=default_probability_estimator_args,
probability_estimator=mlc.LogisticRegression,
response_col=’response’ )</div></blockquote>
<p>Represent the photo in the given colorspace.</p>
</dd></dl>

<dl class="method">
<dt id="propensity_matching.estimator.PropensityEstimator.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>df: pyspark.sql.DataFrame</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/propensity_matching/estimator.html#PropensityEstimator.fit"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#propensity_matching.estimator.PropensityEstimator.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>return PropensityModel</p>
</dd></dl>

<dl class="attribute">
<dt id="propensity_matching.estimator.PropensityEstimator.default_fit_data_prep_args">
<code class="descname">default_fit_data_prep_args</code><em class="property"> = {'bin_features': True, 'class_balance': 1, 'remove_redundant_features': True, 'train_prop': 0.8}</em><a class="headerlink" href="#propensity_matching.estimator.PropensityEstimator.default_fit_data_prep_args" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="propensity_matching.estimator.PropensityEstimator.default_probability_estimator_args">
<code class="descname">default_probability_estimator_args</code><em class="property"> = {'elasticNetParam': 0, 'family': 'binomial', 'featuresCol': 'features', 'fitIntercept': True, 'labelCol': 'label', 'maxIter': 10, 'predictionCol': 'prediction', 'probabilityCol': 'probability', 'regParam': 0.2}</em><a class="headerlink" href="#propensity_matching.estimator.PropensityEstimator.default_probability_estimator_args" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt>
<code class="descname">fit</code><span class="sig-paren">(</span><em>df: pyspark.sql.dataframe.DataFrame</em><span class="sig-paren">)</span> &#x2192; Tuple[propensity_matching.model.PropensityModel, pyspark.sql.dataframe.DataFrame]<a class="reference internal" href="_modules/propensity_matching/estimator.html#PropensityEstimator.fit"><span class="viewcode-link">[source]</span></a></dt>
<dd><p>Fit propensity model and return.</p>
<p>Must prepare df and fit probability model from estimator.
df is rebalanced and, if necessary, features are adjusted.
will fail if df is too small or has too few positive samples</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>df</strong> (<em>pyspark.sql.DataFrame</em>) – dataframe containing desired data. Must have predictor columns
as well as features, label column specificed in
propensity_estimator_args and response col given in __init__</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><ul class="simple">
<li><strong>model</strong> (<em>PropensityModel</em>) – ml.Model object for propensity matching.</li>
<li><strong>df</strong> (<em>DataFrame</em>) – adjusted dataframe</li>
</ul>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Raises:</th><td class="field-body"><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#AssertionError" title="(in Python v3.6)"><code class="xref py py-exc docutils literal notranslate"><span class="pre">AssertionError</span></code></a> – df too small
too few positive samples</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Uncaught Errors:</dt>
<dd>invalid param args.</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-propensity_matching.evaluate">
<span id="propensity-matching-evaluate-module"></span><h2>propensity_matching.evaluate module<a class="headerlink" href="#module-propensity_matching.evaluate" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="propensity_matching.evaluate.evaluate">
<code class="descclassname">propensity_matching.evaluate.</code><code class="descname">evaluate</code><span class="sig-paren">(</span><em>prob_mod: Type[pyspark.ml.base.Model], pre_df: Union[pyspark.sql.dataframe.DataFrame, NoneType] = None, post_df: Union[pyspark.sql.dataframe.DataFrame, NoneType] = None, test_df: Union[pyspark.sql.dataframe.DataFrame, NoneType] = None, train_df: Union[pyspark.sql.dataframe.DataFrame, NoneType] = None, transform_df: Union[pyspark.sql.dataframe.DataFrame, NoneType] = None</em><span class="sig-paren">)</span> &#x2192; collections.performance_summary<a class="reference internal" href="_modules/propensity_matching/evaluate.html#evaluate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#propensity_matching.evaluate.evaluate" title="Permalink to this definition">¶</a></dt>
<dd><p>evaluates the goodness of match and the power of the propensity
model</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>prob_mod</strong> (<em>pyspark.ml.Model</em>) – pyspark model that predict probability row is in postive class
1 in label col</li>
<li><strong>pre_df</strong> (<em>pyspark.sql.DataFrame</em>) – dataframe before matching. used to assess starting bias. If there
are no positive samples, all goodness of match metrics return None</li>
<li><strong>post_df</strong> (<em>pyspark.sql.DataFrame</em>) – dataframe after matching used to assess final bias. If there
are no positive samples, all goodness of match metrics return None</li>
<li><strong>test_df</strong> (<em>pyspark.sql.DataFrame</em>) – test dataset for <cite>prob_mod</cite>. if <cite>None</cite>, test_prob_mod_perf returns
<cite>None</cite></li>
<li><strong>train_df</strong> (<em>pyspark.sql.DataFrame</em>) – dataframe <cite>prob_mod</cite> was trained on. if <cite>None</cite>, train_prob_mod_perf
returns <cite>None</cite></li>
<li><strong>transform_df</strong> (<em>pyspark.sql.DataFrame</em>) – the whole dataframe. Often a superset of <cite>train_df</cite> and <cite>test_df</cite>
also frequently has different class balance
if <cite>None</cite>, transform_prob_mod_perf returns <cite>None</cite></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>performance_summary</strong></p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">performance_summary named tuple</p>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Notes</p>
<dl class="docutils">
<dt>performance_summary <span class="classifier-delimiter">:</span> <span class="classifier">namedtuple</span></dt>
<dd><p class="first">‘test_prob_mod_perf’: propensity_model_performance_summary
‘train_prob_mod_perf’ : propensity_model_performance_summary
‘transform_prob_mod_perf’ : propensity_model_performance_summary
‘bias_df’: pd.DataFrame</p>
<blockquote>
<div>for each col has pre, post, absolute reduce, relative
reduced bias</div></blockquote>
<dl class="last docutils">
<dt>‘total_bias_reduced’: float</dt>
<dd>1 - (sum postbias of features/ sum rebias of features)</dd>
<dt>‘starting_bias_mean’: float</dt>
<dd>mean of prebias</dd>
<dt>‘starting_bias_var’: float</dt>
<dd>var of prebias</dd>
</dl>
</dd>
<dt>propensity_model_performance_summary <span class="classifier-delimiter">:</span> <span class="classifier">namedtuple</span></dt>
<dd><p class="first">‘auc’ : float
‘auprc’ : float</p>
<blockquote>
<div>area under precision recall curve</div></blockquote>
<p class="last">‘threshold’ : float
‘informativeness’ (f1) : float
‘precision’ : float
‘recall’ : float
‘accuracy’  : float</p>
</dd>
</dl>
<p>Bias is calculated on predictors which may or may not be the originally
provided data (e.g. binning, dropping predictors). This is wad but user
should be aware.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Raises:</th><td class="field-body"><ul class="first last simple">
<li><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.6)"><code class="xref py py-exc docutils literal notranslate"><span class="pre">ValueError</span></code></a> – if all model metric dfs are none</li>
<li><code class="xref py py-exc docutils literal notranslate"><span class="pre">UncaughtErrors</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="admonition seealso">
<p class="first admonition-title">See also</p>
<p class="last"><code class="xref py py-func docutils literal notranslate"><span class="pre">_eval_propensity_model()</span></code>, <code class="xref py py-func docutils literal notranslate"><span class="pre">_eval_match_performance()</span></code></p>
</div>
</dd></dl>

</div>
<div class="section" id="module-propensity_matching.impact">
<span id="propensity-matching-impact-module"></span><h2>propensity_matching.impact module<a class="headerlink" href="#module-propensity_matching.impact" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="propensity_matching.impact.impact">
<code class="descclassname">propensity_matching.impact.</code><code class="descname">impact</code><span class="sig-paren">(</span><em>df: pyspark.sql.dataframe.DataFrame, response_col: str, prob_mod: Type[pyspark.ml.base.Model]</em><span class="sig-paren">)</span> &#x2192; Tuple[float, float, float]<a class="reference internal" href="_modules/propensity_matching/impact.html#impact"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#propensity_matching.impact.impact" title="Permalink to this definition">¶</a></dt>
<dd><p>observe impact of treatment on response variable</p>
<p>currently response must be binary
if the df is small enough return naive difference in groupby label
response mean. otherwise do additional regression on response col
with label as predictor and use its coefficient as a measure of its
impact. binning and dimensionality reduction will occur if necessary
to do an effective regression</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>df</strong> (<em>pyspark.sql.DataFrame</em>) – </li>
<li><strong>response_col</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) – </li>
<li><strong>prob_mod</strong> (<em>Type</em><em>[</em><em>mlc.Model</em><em>]</em>) – propensity model, mostly used to keep track of feature_col,
label_col, pred_cols</li>
<li><strong>binned</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) – where the predictor cols binned ?</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><ul class="simple">
<li><strong>treatment_rate</strong> (<em>float</em>) – treatment response rate</li>
<li><strong>control_rate</strong> (<em>float</em>) – control response rate</li>
<li><strong>adjusted_response</strong> (<em>float</em>) – impact of treatment on response, which may be
<cite>control_rate</cite>-<cite>treatment_rate</cite> or may have further bias adjustement</li>
</ul>
</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Raises:</th><td class="field-body"><ul class="first last simple">
<li><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.6)"><code class="xref py py-exc docutils literal notranslate"><span class="pre">ValueError</span></code></a> – when number of rows is less than <a href="#id1"><span class="problematic" id="id2">`</span></a>MINIMUM_POS_COUNT`*2</li>
<li><code class="xref py py-exc docutils literal notranslate"><span class="pre">UncaughtExceptions</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="admonition seealso">
<p class="first admonition-title">See also</p>
<p class="last"><code class="xref py py-func docutils literal notranslate"><span class="pre">bin_features()</span></code>, <code class="xref py py-func docutils literal notranslate"><span class="pre">_reduce_dimensionality()</span></code></p>
</div>
<p class="rubric">Notes</p>
</dd></dl>

</div>
<div class="section" id="module-propensity_matching.model">
<span id="propensity-matching-model-module"></span><h2>propensity_matching.model module<a class="headerlink" href="#module-propensity_matching.model" title="Permalink to this headline">¶</a></h2>
<p>Defines PropensityModel</p>
<dl class="class">
<dt id="propensity_matching.model.PropensityModel">
<em class="property">class </em><code class="descclassname">propensity_matching.model.</code><code class="descname">PropensityModel</code><span class="sig-paren">(</span><em>prob_mod</em>, <em>df</em>, <em>train_set</em>, <em>test_set</em>, <em>response_col</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/propensity_matching/model.html#PropensityModel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#propensity_matching.model.PropensityModel" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.6)"><code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></a></p>
<p>The entry point for transform, impact, and evaluate workflows.</p>
<dl class="docutils">
<dt>prob_mod <span class="classifier-delimiter">:</span> <span class="classifier">pyspark.ml.classification.LogisticRegressionModel</span></dt>
<dd>Model obj to predict probability of being in label class 1
prob_mod.pred_cols houses feature columns names
getters are also used to for label and assembled features col</dd>
<dt>df <span class="classifier-delimiter">:</span> <span class="classifier">pyspark.sql.DataFrame</span></dt>
<dd>The actual data</dd>
<dt>train_set <span class="classifier-delimiter">:</span> <span class="classifier">pyspark.sql.DataFrame</span></dt>
<dd>data used to train prob_mod</dd>
<dt>test_set <span class="classifier-delimiter">:</span> <span class="classifier">pyspark.sql.DataFrame</span></dt>
<dd>data used to test prob_mod</dd>
<dt>response_col <span class="classifier-delimiter">:</span> <span class="classifier">str</span></dt>
<dd>col holding the response variable</dd>
</dl>
<dl class="method">
<dt id="propensity_matching.model.PropensityModel.transform">
<code class="descname">transform</code><span class="sig-paren">(</span><em>df</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/propensity_matching/model.html#PropensityModel.transform"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#propensity_matching.model.PropensityModel.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Represent the photo in the given colorspace.</p>
</dd></dl>

<dl class="method">
<dt id="propensity_matching.model.PropensityModel.determine_impact">
<code class="descname">determine_impact</code><span class="sig-paren">(</span><em>df</em>, <em>matched_treatment</em>, <em>matched_control</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/propensity_matching/model.html#PropensityModel.determine_impact"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#propensity_matching.model.PropensityModel.determine_impact" title="Permalink to this definition">¶</a></dt>
<dd><p>Change the photo’s gamma exposure.</p>
</dd></dl>

<dl class="method">
<dt id="propensity_matching.model.PropensityModel.evaluate_performance">
<code class="descname">evaluate_performance</code><span class="sig-paren">(</span><em>pre_df</em>, <em>post_df</em>, <em>transform_df</em>, <em>by_col_group</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/propensity_matching/model.html#PropensityModel.evaluate_performance"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#propensity_matching.model.PropensityModel.evaluate_performance" title="Permalink to this definition">¶</a></dt>
<dd><p>Change the photo’s gamma exposure.</p>
</dd></dl>

<dl class="method">
<dt>
<code class="descname">determine_impact</code><span class="sig-paren">(</span><em>df: pyspark.sql.dataframe.DataFrame</em><span class="sig-paren">)</span> &#x2192; Tuple[float, float, float]<a class="reference internal" href="_modules/propensity_matching/model.html#PropensityModel.determine_impact"><span class="viewcode-link">[source]</span></a></dt>
<dd><p>Calculates effect of label col on response col, controlling
for covariates</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>df</strong> (<em>pyspark.sql.DataFrame</em>) – </td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><ul class="simple">
<li><strong>treatment_rate</strong> (<em>float</em>) – % of matched class 1s that have response 1 (as opposed to 0)</li>
<li><strong>control_rate</strong> (<em>float</em>) – % of matched class 1s that have response 1 (as opposed to 0)</li>
<li><strong>adjusted_response</strong> (<em>float</em>) – impact of label on reponse col, with further adjustments for bias</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="admonition seealso">
<p class="first admonition-title">See also</p>
<p class="last"><code class="xref py py-meth docutils literal notranslate"><span class="pre">impact()</span></code></p>
</div>
<p class="rubric">Examples</p>
</dd></dl>

<dl class="method">
<dt>
<code class="descname">evaluate_performance</code><span class="sig-paren">(</span><em>pre_df</em>, <em>post_df</em>, <em>transform_df</em><span class="sig-paren">)</span> &#x2192; &lt;function namedtuple at 0x00000236A26D0158&gt;<a class="reference internal" href="_modules/propensity_matching/model.html#PropensityModel.evaluate_performance"><span class="viewcode-link">[source]</span></a></dt>
<dd><p>provides goodness metrics for propensity match</p>
<p>Considers both the probability model as well as the matching itself</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>pre_df</strong> (<em>pyspark.sql.DataFrame</em>) – dataframe before the propensity matching. used to calculate
starting standard bias</li>
<li><strong>post_df</strong> (<em>pyspark.sql.DataFrame</em>) – dataframe after propensity matching. used to calculate ending
standard bias</li>
<li><strong>transform_df</strong> (<em>pyspark.sql.DataFrame</em>) – df transformed by probability model. used to calculate model
goodness metrics on whole dataframe, as opposed to class
balances test and train sets</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><p><strong>performance_summary</strong> – ‘test_prob_mod_perf’: propensity_model_performance_summary
‘train_prob_mod_perf’ : propensity_model_performance_summary
‘transform_prob_mod_perf’ : propensity_model_performance_summary
‘bias_df’: pd.DataFrame</p>
<blockquote>
<div><p>for each col has pre, post, absolute reduce, relative
reduced bias</p>
</div></blockquote>
<dl class="docutils">
<dt>’total_bias_reduced’: float</dt>
<dd><p class="first last">1 - (sum postbias of features/ sum rebias of features)</p>
</dd>
<dt>’starting_bias_mean’: float</dt>
<dd><p class="first last">mean of prebias</p>
</dd>
<dt>’starting_bias_var’: float</dt>
<dd><p class="first last">var of prebias</p>
</dd>
<dt>where</dt>
<dd><dl class="first last docutils">
<dt>propensity_model_performance_summary <span class="classifier-delimiter">:</span> <span class="classifier">namedtuple</span></dt>
<dd><p class="first">’auc’ : float
‘auprc’ : float</p>
<blockquote>
<div><p>area under precision recall curve</p>
</div></blockquote>
<p class="last">’threshold’ : float
‘informativeness’ (f1) : float
‘precision’ : float
‘recall’ : float
‘accuracy’  : float</p>
</dd>
</dl>
</dd>
</dl>
</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">namedtuple</p>
</td>
</tr>
</tbody>
</table>
<div class="admonition seealso">
<p class="first admonition-title">See also</p>
<p class="last"><code class="xref py py-meth docutils literal notranslate"><span class="pre">evaluate()</span></code></p>
</div>
<p class="rubric">Examples</p>
</dd></dl>

<dl class="method">
<dt>
<code class="descname">transform</code><span class="sig-paren">(</span><em>df: pyspark.sql.dataframe.DataFrame</em><span class="sig-paren">)</span> &#x2192; Tuple[pyspark.sql.dataframe.DataFrame, dict]<a class="reference internal" href="_modules/propensity_matching/model.html#PropensityModel.transform"><span class="viewcode-link">[source]</span></a></dt>
<dd><p>A one-line summary that does not use variable names or the
function name.</p>
<p>Several sentences providing an extended description. Refer to
variables using back-ticks, e.g. <cite>var</cite>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>df</strong> (<em>pyspark.sql.DataFrame</em>) – full dataframe to propensity_match on</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><ul class="simple">
<li><strong>df</strong> (<em>pyspark.sql.DataFrame</em>) – matched observations</li>
<li><strong>match_info</strong> (<em>dict</em>) – depending on matching, contains information about the match</li>
</ul>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Raises:</th><td class="field-body"><code class="xref py py-exc docutils literal notranslate"><span class="pre">UncaughtExceptiosn</span></code></td>
</tr>
</tbody>
</table>
<div class="admonition seealso">
<p class="first admonition-title">See also</p>
<p class="last"><a class="reference internal" href="#propensity_matching.model.PropensityModel.transform" title="propensity_matching.model.PropensityModel.transform"><code class="xref py py-meth docutils literal notranslate"><span class="pre">transform()</span></code></a></p>
</div>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-propensity_matching.transform">
<span id="propensity-matching-transform-module"></span><h2>propensity_matching.transform module<a class="headerlink" href="#module-propensity_matching.transform" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="propensity_matching.transform.transform">
<code class="descclassname">propensity_matching.transform.</code><code class="descname">transform</code><span class="sig-paren">(</span><em>df: pyspark.sql.dataframe.DataFrame, prob_mod: Type[pyspark.ml.base.Model], method: Union[str, NoneType] = None, metric: Union[str, NoneType] = None, match_kwargs: Union[dict, NoneType] = None</em><span class="sig-paren">)</span> &#x2192; Tuple[pyspark.sql.dataframe.DataFrame, dict]<a class="reference internal" href="_modules/propensity_matching/transform.html#transform"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#propensity_matching.transform.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a propensity matched dataset</p>
<p>Public entry point point for transform. Depending on arguments
will point to the appropriate metric and matching methodology</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>df</strong> (<em>pyspark.sql.DataFrame</em>) – Dataframe with population in question. Must have featureCol and
labelCol used in <cite>prob_mod</cite></li>
<li><strong>prob_mod</strong> (<em>Type</em><em>[</em><em>mlc.Model</em><em>]</em>) – the model predicting the probability that the row is in class 1
in the label col.</li>
<li><strong>method</strong> (<em>{'auto'</em><em>, </em><em>'quantile'</em><em>, </em><em>'assignment'}</em>) – <p>how matching occurs. auto will select according to the number of
rows specified config as <cite>SMALL_MATCH_THRESHOLD</cite></p>
<p>Quantile does stratified sampling on predicted probability.
It guarantees similar population sizes and may drop some treatments
non-symmetrically in order to fulfill that guarantee. match_info
contains ‘scale’, what proportion of treatment users were used, and
‘dropped’, proportion of sample dropped asymmetrically. The
algorithm tries to maintain a balance between sample size and
bias in deciding scale and droppped</p>
</li>
<li><strong>metric</strong> (<em>{'probability'}</em>) – the number that is being matched. Currently only support predicted
probability but may add more in the future</li>
<li><strong>match_kwargs</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.6)"><em>dict</em></a><em>, </em><em>optional</em>) – additional kwargs for match algorithm.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><ul class="simple">
<li><strong>df</strong> (<em>pyspark.sql.DataFrame</em>) – df with only matched populations ( so dont overwrite your parent
dataframe if you need it!)</li>
<li><strong>match_info</strong> (<em>dict</em>) – information about that particular match depending on the algorithm
chosen.</li>
</ul>
</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Raises:</th><td class="field-body"><p class="first last"><code class="xref py py-exc docutils literal notranslate"><span class="pre">UncaughtExceptions</span></code></p>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Notes</p>
<p>Currently just a wrapper for _transform. Implemented like this so
multiple potential _transform outputs could be run and the best one
returned by transform. Collecting, coordinating, and evaluation those
_transform outputs would be done in transform.</p>
</dd></dl>

</div>
<div class="section" id="module-propensity_matching.utils">
<span id="propensity-matching-utils-module"></span><h2>propensity_matching.utils module<a class="headerlink" href="#module-propensity_matching.utils" title="Permalink to this headline">¶</a></h2>
<p>Utility functions that may be useful at any stage.</p>
<dl class="function">
<dt id="propensity_matching.utils.bin_features">
<code class="descclassname">propensity_matching.utils.</code><code class="descname">bin_features</code><span class="sig-paren">(</span><em>df: pyspark.sql.dataframe.DataFrame, features_col: str, ntiles: Union[Dict[str, int], int] = 5, error_scale: Union[int, float] = 10, sample_num: Union[int, NoneType] = 100000</em><span class="sig-paren">)</span> &#x2192; Tuple[pyspark.sql.dataframe.DataFrame, List[str]]<a class="reference internal" href="_modules/propensity_matching/utils.html#bin_features"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#propensity_matching.utils.bin_features" title="Permalink to this definition">¶</a></dt>
<dd><p>use quantiles to bin numeric features into interval/ordinal</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>df</strong> (<em>pyspark.sql.DataFrame</em>) – dataframe containing the relevant columns</li>
<li><strong>cols</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) – the columns to be binned</li>
<li><strong>ntiles</strong> (<em>[</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.6)"><em>dict</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em>]</em>) – either col:ntile dict or single int for all columns. The number
of equal by count divisions the column should be binned into</li>
<li><strong>features_col</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) – the column to assemble the new binned features into.
dropped if already in <cite>df</cite></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><ul class="simple">
<li><strong>dataframe</strong> (<em>pyspark.sql.DataFrame</em>) – input dataframe with new columns</li>
<li><strong>binned_cols</strong> (<em>List[str]</em>) – List of new column names</li>
</ul>
</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name" colspan="2">Other Parameters:</th></tr>
<tr class="field-odd field"><td>&#160;</td><td class="field-body"><ul class="first simple">
<li><strong>error_scale</strong> (<em>Union[float, int], Optional:</em>) – 1/ntile/error_scale is the errorTolerance passed to
approxQuantile when calculating quantile bin thresholds.
Default value is 10</li>
<li><strong>sample_num</strong> (<em>int, Optional</em>) – number of rows to consider when subsampling to calculate quantiles.
max of 100,000 by default
if None whole dataframe will be used</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Raises:</th><td class="field-body"><p class="first last"><em>Uncaught exceptions</em> – Illegal input values, esp non-whole, negative,
or otherwise unreasonable numbers for ntile
Name conflicts for new “binned{<cite>col</cite>}” columns</p>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Notes</p>
<p>If data is ‘0 degenerate’, where the number of minimum values
is greater than a single ntile division, make them the bottom bin and
squeeze the rest of the divisions in the remaining space. Lose the
interval guarantee but is useful in practice</p>
</dd></dl>

<dl class="function">
<dt id="propensity_matching.utils.reduce_chi_dimensionality">
<code class="descclassname">propensity_matching.utils.</code><code class="descname">reduce_chi_dimensionality</code><span class="sig-paren">(</span><em>df: pyspark.sql.dataframe.DataFrame</em>, <em>label_col: str</em>, <em>binned_features_col: str</em>, <em>ncols: Union[int</em>, <em>NoneType] = None</em>, <em>drop_uninformative: bool = True</em>, <em>sample_size: Union[int</em>, <em>NoneType] = 10000</em><span class="sig-paren">)</span> &#x2192; Tuple[pyspark.sql.dataframe.DataFrame, List[str]]<a class="reference internal" href="_modules/propensity_matching/utils.html#reduce_chi_dimensionality"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#propensity_matching.utils.reduce_chi_dimensionality" title="Permalink to this definition">¶</a></dt>
<dd><p>Use chi-squared to pick the most informative features</p>
<p>Find the most informative features using chi squared. Input features
should be categorical. Uninformative features may dropped using fwe.
Under-the-hood multiple chi squared tests are run w/ <cite>sample_size</cite>
to account for data limitations</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>df</strong> (<em>pyspark.sql.DataFrame</em>) – </li>
<li><strong>label_col</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) – colname of label column. should be 1 or 0 with smaller treatment
class as 1</li>
<li><strong>binned_features_col</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) – column of predictors assembled into a vector</li>
<li><strong>ncols</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em>, </em><em>Optional</em>) – number of desired output columns. If unspecified, will default to
the number of positive samples/<cite>SAMPLES_PER_FEATURE</cite></li>
<li><strong>drop_uninformative</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a><em>, </em><em>Optional</em>) – defaults to True. uses fwe w/ significance of .05 to drop
uninformative columns</li>
<li><strong>sample_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em>, </em><em>Optional</em>) – sample size used in each run to evaluate significance of each
predictor. defaults to 10**4. This threshold was chosen empirically
can be passed as None to avoid sampling ( not recommended)</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><ul class="simple">
<li><strong>df</strong> (<em>pyspark.sql.DataFrame</em>) – input dataframe, but <cite>binned_features_col</cite> now consists of selected
columns.</li>
<li><strong>selected_cols</strong> (<em>List[str]</em>) – list of chosen columns</li>
</ul>
</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Raises:</th><td class="field-body"><ul class="first last simple">
<li><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#AssertionError" title="(in Python v3.6)"><code class="xref py py-exc docutils literal notranslate"><span class="pre">AssertionError</span></code></a> – if <cite>ncols</cite> is less than 1, either through calculation or
specification
if no columns are informative</li>
<li><code class="xref py py-exc docutils literal notranslate"><span class="pre">UncaughtExceptions</span></code> – non-int values for ncols</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Notes</p>
<p>using ChiSqSelector with many samples will make p-values of
columns in internal calculations identical. Sorting by p-value and
returning therefore relies on input order. This method
circumvents that by subsampling to <cite>sample_size</cite> and running
int(math.ceil(math.log(all_count/sample_size))) times
(or 1 when df size is less than sample_size). Each iteration,
the rank for each column is calculated and added to previous ranks.
The top ncol columns are chosen with the smallest aggregate rank.</p>
</dd></dl>

<dl class="function">
<dt id="propensity_matching.utils.reduce_dimensionality">
<code class="descclassname">propensity_matching.utils.</code><code class="descname">reduce_dimensionality</code><span class="sig-paren">(</span><em>args: dict</em>, <em>method: str = 'chi'</em><span class="sig-paren">)</span> &#x2192; Tuple[pyspark.sql.dataframe.DataFrame, List[str]]<a class="reference internal" href="_modules/propensity_matching/utils.html#reduce_dimensionality"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#propensity_matching.utils.reduce_dimensionality" title="Permalink to this definition">¶</a></dt>
<dd><p>entry point for dimensionality reduction functions.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>args</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.6)"><em>dict</em></a>) – kwargs dicts w/ arguments for chi or log dim reduction method</li>
<li><strong>method</strong> (<em>{'chi'</em><em>, </em><em>log'}</em>) – which dim reduction method to use</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="admonition seealso">
<p class="first admonition-title">See also</p>
<p class="last"><a class="reference internal" href="#propensity_matching.utils.reduce_chi_dimensionality" title="propensity_matching.utils.reduce_chi_dimensionality"><code class="xref py py-func docutils literal notranslate"><span class="pre">reduce_chi_dimensionality()</span></code></a>, <a class="reference internal" href="#propensity_matching.utils.reduce_log_dimensionality" title="propensity_matching.utils.reduce_log_dimensionality"><code class="xref py py-func docutils literal notranslate"><span class="pre">reduce_log_dimensionality()</span></code></a></p>
</div>
</dd></dl>

<dl class="function">
<dt id="propensity_matching.utils.reduce_log_dimensionality">
<code class="descclassname">propensity_matching.utils.</code><code class="descname">reduce_log_dimensionality</code><span class="sig-paren">(</span><em>df: pyspark.sql.dataframe.DataFrame</em>, <em>label_col: str</em>, <em>binned_features_col: str</em>, <em>ncols: Union[int</em>, <em>NoneType] = None</em>, <em>sample_size: Union[int</em>, <em>NoneType] = 1000000</em>, <em>log_args: Union[dict</em>, <em>NoneType] = None</em><span class="sig-paren">)</span> &#x2192; Tuple[pyspark.sql.dataframe.DataFrame, List[str]]<a class="reference internal" href="_modules/propensity_matching/utils.html#reduce_log_dimensionality"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#propensity_matching.utils.reduce_log_dimensionality" title="Permalink to this definition">¶</a></dt>
<dd><p>use logistic regression on normalized predictors to find top
ncols with greatest absolute beta coefficients</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>df</strong> (<em>pyspark.sql.DataFrame</em>) – </li>
<li><strong>binned_pred_cols</strong> (<em>List</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a><em>]</em>) – list of normalized predictors assembled in <cite>binned_features_col</cite></li>
<li><strong>label_col</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) – colname with class labels. Should be binary, with 1 being the
smaller treatment class</li>
<li><strong>binned_features_col</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) – colname of categorical predictors assembled into a vector</li>
<li><strong>ncols</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em>, </em><em>optional</em>) – defaults to positive_samples/<cite>SAMPLES_PER_FEATURE</cite>
number of columns to return</li>
<li><strong>sample_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em>, </em><em>optional</em>) – defaults to 10**6, size of sample to train logistic regression on
can be passed as None to avoid sampling</li>
<li><strong>log_args</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.6)"><em>dict</em></a><em>, </em><em>optional</em>) – kwargs dict for pyspark.ml.classification.LogisticRegression
estimator initialization. Otherwise specifies label, feature col
and uses pyspark defaults. must include featureCol and labelCol
arguments. label_col, binned_features_col args will be ignored
but must still be passed</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><ul class="simple">
<li><strong>df</strong> (<em>pyspark.sql.DataFrame</em>) – input df but <cite>binned_features_cols</cite> is now a vector of chosen columns</li>
<li><strong>selected_cols</strong> (<em>List[str]</em>) – list of chosen columns</li>
</ul>
</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Raises:</th><td class="field-body"><ul class="first last simple">
<li><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#AssertionError" title="(in Python v3.6)"><code class="xref py py-exc docutils literal notranslate"><span class="pre">AssertionError</span></code></a> – ncol less than 1 by calculation or specification</li>
<li><code class="xref py py-exc docutils literal notranslate"><span class="pre">UncaughtExceptions</span></code> – non-int vals for ncol</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Notes</p>
<p>Assumes input variables have been normalized. Train log regression and
picks <cite>ncols</cite> with greatest absolute coefficient. Does not guarantee
this combination of columns will have greatest predictive power,
especially in cases with high-impact low-presence predictors.</p>
</dd></dl>

<dl class="function">
<dt id="propensity_matching.utils.remove_redundant_features">
<code class="descclassname">propensity_matching.utils.</code><code class="descname">remove_redundant_features</code><span class="sig-paren">(</span><em>df: pyspark.sql.dataframe.DataFrame</em>, <em>features_col: str</em>, <em>sample_num: int = 100000</em>, <em>method: str = 'ward'</em>, <em>cluster_thresh: float = 0.1</em><span class="sig-paren">)</span> &#x2192; Tuple[pyspark.sql.dataframe.DataFrame, List[str]]<a class="reference internal" href="_modules/propensity_matching/utils.html#remove_redundant_features"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#propensity_matching.utils.remove_redundant_features" title="Permalink to this definition">¶</a></dt>
<dd><p>Remove redundant or uninformative columns</p>
<p>-Drop cols w/ 0 variance
-Perform hierarchical agglomerative clustering on columns</p>
<blockquote>
<div>where method = <cite>method</cite> and distance = 1-spearman’s corr</div></blockquote>
<dl class="docutils">
<dt>-Create clusters w/ threshold <cite>cluster_thresh</cite></dt>
<dd>&amp; select one column to represent whole cluster.</dd>
</dl>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>df</strong> (<em>pyspark.sql.DataFrame</em>) – The dataframe in question. Must contain all cols in pred_cols</li>
<li><strong>pred_cols</strong> (<em>List</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a><em>]</em>) – List of columns that may be predictors. Must be numeric.</li>
<li><strong>features_col</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) – Name of column that will be the feature column.
May already exist in df, in which case it will be dropped</li>
<li><strong>sample_num</strong> (<em>int=10**5</em><em>, </em><em>optional</em>) – size of sample df used to evaluate columns</li>
<li><strong>method</strong> (<em>{'ward'</em><em>, </em><em>'single'</em><em>, </em><em>'average'</em><em>, </em><em>'weighted'</em><em>, </em><em>median'}</em><em> , </em><em>optional</em>) – see scipy.cluster.hierarchy.linkage for more</li>
<li><strong>cluster_thresh</strong> (<em>float=.1</em>) – distance threshold below which columns are considered a cluster
may be interpreted as 1-correlation</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><ul class="simple">
<li><strong>df</strong> (<em>pyspark.sql.DataFrame</em>) – df with <cite>features_col</cite> assembled from chosen cols</li>
<li><strong>out_cols</strong> (<em>List[str]</em>) – list of chosen cols</li>
</ul>
</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Raises:</th><td class="field-body"><p class="first last">Uncaught Exceptions</p>
</td>
</tr>
</tbody>
</table>
<div class="admonition seealso">
<p class="first admonition-title">See also</p>
<dl class="last docutils">
<dt><code class="xref py py-func docutils literal notranslate"><span class="pre">scipy.cluster.hierarchy()</span></code></dt>
<dd>package with core functionality</dd>
</dl>
</div>
</dd></dl>

</div>
<div class="section" id="module-propensity_matching">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-propensity_matching" title="Permalink to this headline">¶</a></h2>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="propensity_matching_docs.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">propensity_matching package</a><ul>
<li><a class="reference internal" href="#submodules">Submodules</a></li>
<li><a class="reference internal" href="#module-propensity_matching.config">propensity_matching.config module</a></li>
<li><a class="reference internal" href="#module-propensity_matching.estimator">propensity_matching.estimator module</a></li>
<li><a class="reference internal" href="#module-propensity_matching.evaluate">propensity_matching.evaluate module</a></li>
<li><a class="reference internal" href="#module-propensity_matching.impact">propensity_matching.impact module</a></li>
<li><a class="reference internal" href="#module-propensity_matching.model">propensity_matching.model module</a></li>
<li><a class="reference internal" href="#module-propensity_matching.transform">propensity_matching.transform module</a></li>
<li><a class="reference internal" href="#module-propensity_matching.utils">propensity_matching.utils module</a></li>
<li><a class="reference internal" href="#module-propensity_matching">Module contents</a></li>
</ul>
</li>
</ul>

  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/propensity_matching.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="nav-item nav-item-0"><a href="propensity_matching_docs.html">propensity_matching  documentation</a> &#187;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2018, ruaghaya.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.7.4.
    </div>
  </body>
</html>