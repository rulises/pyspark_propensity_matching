
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>propensity_matching.utils &#8212; propensity_matching  documentation</title>
    <link rel="stylesheet" href="../../_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <script type="text/javascript" src="../../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../propensity_matching_docs.html">propensity_matching  documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../index.html" accesskey="U">Module code</a> &#187;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <h1>Source code for propensity_matching.utils</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;Utility functions that may be useful at any stage.&quot;&quot;&quot;</span>

<span class="kn">from</span> <span class="nn">collections</span> <span class="k">import</span> <span class="n">defaultdict</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="k">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Union</span>
<span class="kn">import</span> <span class="nn">math</span>

<span class="kn">from</span> <span class="nn">pyspark</span> <span class="k">import</span> <span class="n">StorageLevel</span>
<span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="k">import</span> <span class="n">DataFrame</span>
<span class="kn">import</span> <span class="nn">pyspark.sql.functions</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">pyspark.sql.types</span> <span class="k">as</span> <span class="nn">T</span>
<span class="kn">import</span> <span class="nn">pyspark.ml.feature</span> <span class="k">as</span> <span class="nn">mlf</span>
<span class="kn">import</span> <span class="nn">pyspark.ml.classification</span> <span class="k">as</span> <span class="nn">mlc</span>
<span class="kn">from</span> <span class="nn">pyspark.mllib</span> <span class="k">import</span> <span class="n">stat</span> <span class="k">as</span> <span class="n">mllibs</span>
<span class="kn">from</span> <span class="nn">pyspark.mllib.linalg</span> <span class="k">import</span> <span class="n">Vectors</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">scipy.spatial.distance</span> <span class="k">import</span> <span class="n">squareform</span>
<span class="kn">from</span> <span class="nn">scipy.cluster.hierarchy</span> <span class="k">import</span> <span class="n">linkage</span><span class="p">,</span> <span class="n">fcluster</span>

<span class="kn">from</span> <span class="nn">.config</span> <span class="k">import</span> <span class="n">SAMPLES_PER_FEATURE</span>


<span class="k">def</span> <span class="nf">_get_pred_cols</span><span class="p">(</span><span class="n">df</span><span class="p">:</span> <span class="n">DataFrame</span><span class="p">,</span> <span class="n">features_col</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">approved_types</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot; given a dataframe and an assembled feature column, return the</span>
<span class="sd">    feature names in order</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    df : pyspark.sql.DataFrame</span>
<span class="sd">        The dataframe in question.</span>
<span class="sd">    features_col : str</span>
<span class="sd">        colname, must be vector assembled and in df</span>
<span class="sd">    approved_types : List[str], optional</span>
<span class="sd">        list of types to return if they are in features_col</span>
<span class="sd">        defaults to [&#39;numeric&#39;, &#39;binary&#39;]</span>


<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    pred_cols : List[str]</span>
<span class="sd">        list of columns names assembled into `features_col` in the order</span>
<span class="sd">        they were assembled</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">approved_types</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">approved_types</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;numeric&#39;</span><span class="p">,</span> <span class="s1">&#39;binary&#39;</span><span class="p">]</span>

    <span class="n">meta</span> <span class="o">=</span> <span class="p">[</span><span class="n">f</span><span class="o">.</span><span class="n">metadata</span>
            <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">schema</span><span class="o">.</span><span class="n">fields</span>
            <span class="k">if</span> <span class="n">f</span><span class="o">.</span><span class="n">name</span> <span class="o">==</span> <span class="n">features_col</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>

    <span class="n">types</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">meta</span><span class="p">[</span><span class="s1">&#39;ml_attr&#39;</span><span class="p">][</span><span class="s1">&#39;attrs&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>

    <span class="n">construction_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>

    <span class="k">for</span> <span class="nb">type</span> <span class="ow">in</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">types</span> <span class="k">if</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">approved_types</span><span class="p">]:</span>
        <span class="n">construction_list</span> <span class="o">+=</span> <span class="n">meta</span><span class="p">[</span><span class="s1">&#39;ml_attr&#39;</span><span class="p">][</span><span class="s1">&#39;attrs&#39;</span><span class="p">][</span><span class="nb">type</span><span class="p">]</span>

    <span class="n">construction_list_2</span> <span class="o">=</span> <span class="p">[(</span><span class="n">x</span><span class="p">[</span><span class="s1">&#39;idx&#39;</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="s1">&#39;name&#39;</span><span class="p">])</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">construction_list</span><span class="p">]</span>
    <span class="n">sorted_construction_list_2</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">construction_list_2</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">pred_cols</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">sorted_construction_list_2</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">pred_cols</span>


<span class="k">def</span> <span class="nf">_compare_stor_levels</span><span class="p">(</span><span class="n">first</span><span class="p">:</span> <span class="n">StorageLevel</span><span class="p">,</span> <span class="n">second</span><span class="p">:</span> <span class="n">StorageLevel</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot; see if two pyspark storage levels are equal</span>

<span class="sd">    __eq__ is not implemented for StorageLevel. this method goes through</span>
<span class="sd">    each attribute and compares, returning true if all are equal</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">atts</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;useDisk&#39;</span><span class="p">,</span> <span class="s1">&#39;useMemory&#39;</span><span class="p">,</span> <span class="s1">&#39;useOffHeap&#39;</span><span class="p">,</span> <span class="s1">&#39;deserialized&#39;</span><span class="p">,</span> <span class="s1">&#39;replication&#39;</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">att</span> <span class="ow">in</span> <span class="n">atts</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">first</span><span class="p">,</span> <span class="n">att</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">second</span><span class="p">,</span> <span class="n">att</span><span class="p">):</span>
            <span class="k">return</span> <span class="kc">False</span>
    <span class="k">return</span> <span class="kc">True</span>


<span class="k">def</span> <span class="nf">_persist_if_unpersisted</span><span class="p">(</span><span class="n">df</span><span class="p">:</span> <span class="n">DataFrame</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;if dataframe is completely unpersisted, set persistence to mem only</span>
<span class="sd">    return true if persistence is set, false if persistence was already set</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">_compare_stor_levels</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">storageLevel</span><span class="p">,</span> <span class="n">StorageLevel</span><span class="p">(</span><span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="mi">1</span><span class="p">)):</span>
        <span class="n">df</span><span class="o">.</span><span class="n">persist</span><span class="p">(</span><span class="n">StorageLevel</span><span class="p">(</span><span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">))</span>
        <span class="k">return</span> <span class="kc">True</span>
    <span class="k">return</span> <span class="kc">False</span>


<div class="viewcode-block" id="remove_redundant_features"><a class="viewcode-back" href="../../propensity_matching.html#propensity_matching.utils.remove_redundant_features">[docs]</a><span class="k">def</span> <span class="nf">remove_redundant_features</span><span class="p">(</span><span class="n">df</span><span class="p">:</span> <span class="n">DataFrame</span><span class="p">,</span>
                              <span class="n">features_col</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
                              <span class="n">sample_num</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">**</span> <span class="mi">5</span><span class="p">,</span>
                              <span class="n">method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;ward&#39;</span><span class="p">,</span>
                              <span class="n">cluster_thresh</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="o">.</span><span class="mi">1</span><span class="p">,</span>
                              <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Remove redundant or uninformative columns</span>

<span class="sd">    -Drop cols w/ 0 variance</span>
<span class="sd">    -Perform hierarchical agglomerative clustering on columns</span>
<span class="sd">        where method = `method` and distance = 1-spearman&#39;s corr</span>
<span class="sd">    -Create clusters w/ threshold `cluster_thresh`</span>
<span class="sd">        &amp; select one column to represent whole cluster.</span>


<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    df : pyspark.sql.DataFrame</span>
<span class="sd">        The dataframe in question. Must contain all cols in pred_cols</span>
<span class="sd">    pred_cols : List[str]</span>
<span class="sd">        List of columns that may be predictors. Must be numeric.</span>
<span class="sd">    features_col : str</span>
<span class="sd">        Name of column that will be the feature column.</span>
<span class="sd">        May already exist in df, in which case it will be dropped</span>
<span class="sd">    sample_num : int=10**5, optional</span>
<span class="sd">        size of sample df used to evaluate columns</span>
<span class="sd">    method : {&#39;ward&#39;, &#39;single&#39;, &#39;average&#39;, &#39;weighted&#39;, median&#39;} , optional</span>
<span class="sd">        see scipy.cluster.hierarchy.linkage for more</span>
<span class="sd">    cluster_thresh : float=.1</span>
<span class="sd">        distance threshold below which columns are considered a cluster</span>
<span class="sd">        may be interpreted as 1-correlation</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    df : pyspark.sql.DataFrame</span>
<span class="sd">        df with `features_col` assembled from chosen cols</span>
<span class="sd">    out_cols : List[str]</span>
<span class="sd">        list of chosen cols</span>

<span class="sd">    Raises</span>
<span class="sd">    ------</span>
<span class="sd">    Uncaught Exceptions</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    scipy.cluster.hierarchy : package with core functionality</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># sample dataframe of max size sample_num</span>
    <span class="n">pred_cols</span> <span class="o">=</span> <span class="n">_get_pred_cols</span><span class="p">(</span><span class="n">df</span><span class="o">=</span><span class="n">df</span><span class="p">,</span> <span class="n">features_col</span><span class="o">=</span><span class="n">features_col</span><span class="p">)</span>

    <span class="n">count</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">count</span> <span class="o">&lt;</span> <span class="n">sample_num</span><span class="p">:</span>
        <span class="n">sample_df</span> <span class="o">=</span> <span class="n">df</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">frac</span> <span class="o">=</span> <span class="n">sample_num</span> <span class="o">/</span> <span class="n">count</span>
        <span class="n">sample_df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">withReplacement</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">fraction</span><span class="o">=</span><span class="n">frac</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">sample_df</span><span class="o">.</span><span class="n">persist</span><span class="p">(</span><span class="n">StorageLevel</span><span class="p">(</span><span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

    <span class="c1"># create dict of col:variance</span>
    <span class="n">variances_frame</span> <span class="o">=</span> <span class="n">sample_df</span><span class="o">.</span><span class="n">select</span><span class="p">([</span><span class="n">F</span><span class="o">.</span><span class="n">variance</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">pred_cols</span><span class="p">])</span>
    <span class="n">variances_dict</span> <span class="o">=</span> <span class="n">variances_frame</span><span class="o">.</span><span class="n">toPandas</span><span class="p">()</span><span class="o">.</span><span class="n">transpose</span><span class="p">()</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># exclude cols w/o variance (completely degenerate)</span>
    <span class="n">cols</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">col</span><span class="p">,</span> <span class="n">var</span> <span class="ow">in</span> <span class="n">variances_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">var</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">cols</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">col</span><span class="p">)</span>

    <span class="c1"># prep rdd w/ where vals for each row are in a dense Vector</span>
    <span class="n">corr_assembler</span> <span class="o">=</span> <span class="n">mlf</span><span class="o">.</span><span class="n">VectorAssembler</span><span class="p">(</span><span class="n">inputCols</span><span class="o">=</span><span class="n">cols</span><span class="p">,</span> <span class="n">outputCol</span><span class="o">=</span><span class="s1">&#39;corr_features&#39;</span><span class="p">)</span>
    <span class="n">sample_df</span> <span class="o">=</span> <span class="n">corr_assembler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">sample_df</span><span class="p">)</span>
    <span class="n">sample_df</span><span class="o">.</span><span class="n">persist</span><span class="p">(</span><span class="n">StorageLevel</span><span class="p">(</span><span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">sample_rdd</span> <span class="o">=</span> <span class="n">sample_df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s1">&#39;corr_features&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">rdd</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">Vectors</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>

    <span class="c1"># spearman corr for correlation grid because it&#39;s non parametric</span>
    <span class="n">corr_out</span> <span class="o">=</span> <span class="n">mllibs</span><span class="o">.</span><span class="n">Statistics</span><span class="o">.</span><span class="n">corr</span><span class="p">(</span><span class="n">sample_rdd</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;spearman&#39;</span><span class="p">)</span>
    <span class="c1"># since corr is 1-distance, absolute value</span>
    <span class="n">corr_out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">corr_out</span><span class="p">)</span>
    <span class="c1"># sometimes floating point will result in sliiiighlty above 1 corr</span>
    <span class="n">corr_out</span><span class="p">[</span><span class="n">corr_out</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="c1"># clustering distance metric = 1-corr</span>
    <span class="n">corr_array</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">corr_out</span>
    <span class="c1"># diags must be 0 for np.squareform checks</span>
    <span class="n">np</span><span class="o">.</span><span class="n">fill_diagonal</span><span class="p">(</span><span class="n">corr_array</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="c1"># filter out cols where all corr_array vals are null</span>
    <span class="c1"># should be redundant since cols w/ 0 var producing nulls were filtered earlier. left here for safety</span>
    <span class="n">null_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logical_or</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">corr_array</span><span class="p">),</span> <span class="n">corr_array</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">corr_array_n</span> <span class="o">=</span> <span class="n">corr_array</span><span class="p">[</span><span class="o">~</span><span class="n">null_mask</span><span class="p">,</span> <span class="p">:][:,</span> <span class="o">~</span><span class="n">null_mask</span><span class="p">]</span>
    <span class="n">colnames</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">cols</span><span class="p">)[</span><span class="o">~</span><span class="n">null_mask</span><span class="p">]</span>

    <span class="c1"># generate clusters</span>
    <span class="n">condensed_matrix</span> <span class="o">=</span> <span class="n">squareform</span><span class="p">(</span><span class="n">corr_array_n</span><span class="p">,</span> <span class="n">checks</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">linkage</span><span class="p">(</span><span class="n">condensed_matrix</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="n">method</span><span class="p">)</span>
    <span class="c1"># databricks scipy version does not have optimal order arg, uncomment when they update</span>
    <span class="c1"># Z = linkage(condensed_matrix, method=method, optimal_ordering=True)</span>
    <span class="n">clusters</span> <span class="o">=</span> <span class="n">fcluster</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">cluster_thresh</span><span class="p">,</span> <span class="n">criterion</span><span class="o">=</span><span class="s1">&#39;distance&#39;</span><span class="p">)</span>

    <span class="c1"># extract 1 col from each group</span>
    <span class="n">out_cols</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">cluster</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">clusters</span><span class="p">):</span>
        <span class="n">cluster_cols</span> <span class="o">=</span> <span class="n">colnames</span><span class="p">[</span><span class="n">clusters</span> <span class="o">==</span> <span class="n">cluster</span><span class="p">]</span>
        <span class="c1"># pick mid col to best represent whole cluster, needs optimal ordering in flcuster above</span>
        <span class="n">cluster_col</span> <span class="o">=</span> <span class="n">cluster_cols</span><span class="p">[</span><span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">cluster_cols</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)]</span>
        <span class="n">out_cols</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">cluster_col</span><span class="p">))</span>

    <span class="n">feature_assembler</span> <span class="o">=</span> <span class="n">mlf</span><span class="o">.</span><span class="n">VectorAssembler</span><span class="p">(</span><span class="n">inputCols</span><span class="o">=</span><span class="n">out_cols</span><span class="p">,</span> <span class="n">outputCol</span><span class="o">=</span><span class="n">features_col</span><span class="p">)</span>
    <span class="c1"># drop is no op if feature call is absent, not an error</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">feature_assembler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">features_col</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">df</span><span class="p">,</span> <span class="n">out_cols</span></div>


<div class="viewcode-block" id="bin_features"><a class="viewcode-back" href="../../propensity_matching.html#propensity_matching.utils.bin_features">[docs]</a><span class="k">def</span> <span class="nf">bin_features</span><span class="p">(</span><span class="n">df</span><span class="p">:</span> <span class="n">DataFrame</span><span class="p">,</span>
                 <span class="n">features_col</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
                 <span class="n">ntiles</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
                 <span class="n">error_scale</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
                 <span class="n">sample_num</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">**</span> <span class="mi">5</span><span class="p">)</span> \
        <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    use quantiles to bin numeric features into interval/ordinal</span>


<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    df : pyspark.sql.DataFrame</span>
<span class="sd">        dataframe containing the relevant columns</span>
<span class="sd">    cols : int</span>
<span class="sd">        the columns to be binned</span>
<span class="sd">    ntiles : [dict, int]</span>
<span class="sd">        either col:ntile dict or single int for all columns. The number</span>
<span class="sd">        of equal by count divisions the column should be binned into</span>
<span class="sd">    features_col : str</span>
<span class="sd">        the column to assemble the new binned features into.</span>
<span class="sd">        dropped if already in `df`</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    dataframe: pyspark.sql.DataFrame</span>
<span class="sd">        input dataframe with new columns</span>
<span class="sd">    binned_cols : List[str]</span>
<span class="sd">        List of new column names</span>


<span class="sd">    Other Parameters</span>
<span class="sd">    ----------------</span>
<span class="sd">    error_scale: Union[float, int], Optional:</span>
<span class="sd">        1/ntile/error_scale is the errorTolerance passed to</span>
<span class="sd">        approxQuantile when calculating quantile bin thresholds.</span>
<span class="sd">        Default value is 10</span>
<span class="sd">    sample_num: int, Optional</span>
<span class="sd">        number of rows to consider when subsampling to calculate quantiles.</span>
<span class="sd">        max of 100,000 by default</span>
<span class="sd">        if None whole dataframe will be used</span>

<span class="sd">    Raises</span>
<span class="sd">    ------</span>
<span class="sd">    Uncaught exceptions</span>
<span class="sd">        Illegal input values, esp non-whole, negative,</span>
<span class="sd">        or otherwise unreasonable numbers for ntile</span>
<span class="sd">        Name conflicts for new &quot;binned{`col`}&quot; columns</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    If data is &#39;0 degenerate&#39;, where the number of minimum values</span>
<span class="sd">    is greater than a single ntile division, make them the bottom bin and</span>
<span class="sd">    squeeze the rest of the divisions in the remaining space. Lose the</span>
<span class="sd">    interval guarantee but is useful in practice</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># create dictionary of column to number of even splits</span>
    <span class="n">cols</span> <span class="o">=</span> <span class="n">_get_pred_cols</span><span class="p">(</span><span class="n">df</span><span class="o">=</span><span class="n">df</span><span class="p">,</span> <span class="n">features_col</span><span class="o">=</span><span class="n">features_col</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ntiles</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
        <span class="n">ntile_dict</span> <span class="o">=</span> <span class="n">ntiles</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">ntile_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">col</span><span class="p">:</span> <span class="n">ntiles</span> <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">cols</span><span class="p">}</span>

    <span class="n">_persist_if_unpersisted</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
    <span class="n">all_count</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">sample_num</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span> <span class="o">|</span> <span class="p">(</span><span class="n">all_count</span> <span class="o">&lt;=</span> <span class="n">sample_num</span><span class="p">):</span>
        <span class="n">sample_df</span> <span class="o">=</span> <span class="n">df</span>
        <span class="n">sample_count</span> <span class="o">=</span> <span class="n">all_count</span>
    <span class="k">else</span><span class="p">:</span>  <span class="c1"># all_count &gt; sample_num:</span>
        <span class="n">frac</span> <span class="o">=</span> <span class="n">sample_num</span> <span class="o">/</span> <span class="n">all_count</span>
        <span class="n">sample_df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">withReplacement</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">fraction</span><span class="o">=</span><span class="n">frac</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
        <span class="n">sample_df</span><span class="o">.</span><span class="n">persist</span><span class="p">(</span><span class="n">StorageLevel</span><span class="p">(</span><span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">))</span>
        <span class="c1"># not guaranteed to be equal to sample_num</span>
        <span class="n">sample_count</span> <span class="o">=</span> <span class="n">sample_df</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">col</span><span class="p">,</span> <span class="n">ntile</span> <span class="ow">in</span> <span class="n">ntile_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">relative_error</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">ntile</span> <span class="o">/</span> <span class="n">error_scale</span>
        <span class="n">min_val</span> <span class="o">=</span> <span class="n">sample_df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="n">col</span><span class="p">)))</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">min_count</span> <span class="o">=</span> <span class="n">sample_df</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="n">col</span><span class="p">)</span> <span class="o">==</span> <span class="n">min_val</span><span class="p">)</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>

        <span class="c1"># if the number of minimum values is greater than the space in one division, make 1st tile the minimum values</span>
        <span class="c1"># and compress the rest in the remaining space</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">min_count</span> <span class="o">/</span> <span class="n">sample_count</span><span class="p">)</span> <span class="o">&gt;</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">ntile</span><span class="p">):</span>
            <span class="n">probabilities</span> <span class="o">=</span> <span class="p">[</span><span class="nb">float</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">stop</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="n">ntile</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">endpoint</span><span class="o">=</span><span class="kc">False</span><span class="p">)][</span><span class="mi">1</span><span class="p">:]</span>
            <span class="n">threshs</span> <span class="o">=</span> <span class="p">[</span><span class="n">min_val</span><span class="p">]</span> <span class="o">+</span> <span class="n">sample_df</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="n">col</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">min_val</span><span class="p">)</span><span class="o">.</span><span class="n">approxQuantile</span><span class="p">(</span><span class="n">col</span><span class="p">,</span> <span class="n">probabilities</span><span class="p">,</span> <span class="n">relative_error</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">probabilities</span> <span class="o">=</span> <span class="p">[</span><span class="nb">float</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">stop</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="n">ntile</span><span class="p">,</span> <span class="n">endpoint</span><span class="o">=</span><span class="kc">False</span><span class="p">)][</span><span class="mi">1</span><span class="p">:]</span>
            <span class="n">threshs</span> <span class="o">=</span> <span class="n">sample_df</span><span class="o">.</span><span class="n">approxQuantile</span><span class="p">(</span><span class="n">col</span><span class="p">,</span> <span class="n">probabilities</span><span class="p">,</span> <span class="n">relative_error</span><span class="p">)</span>

        <span class="c1"># the number of thresholds it is greater than or equal to is its nthtile</span>
        <span class="k">def</span> <span class="nf">make_udf</span><span class="p">(</span><span class="n">threshs</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">udf</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">sum</span><span class="p">([</span><span class="n">x</span> <span class="o">&gt;</span> <span class="n">y</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">threshs</span><span class="p">]),</span> <span class="n">T</span><span class="o">.</span><span class="n">ShortType</span><span class="p">())</span>

        <span class="c1"># add 1 to conform to mathematical indexing of ntiling</span>
        <span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&quot;binned_</span><span class="si">{col}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">col</span><span class="o">=</span><span class="n">col</span><span class="p">),</span> <span class="n">make_udf</span><span class="p">(</span><span class="n">threshs</span><span class="p">)(</span><span class="n">F</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="n">col</span><span class="p">))</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">binned_cols</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;binned_</span><span class="si">{col}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">col</span><span class="o">=</span><span class="n">col</span><span class="p">)</span> <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">cols</span><span class="p">]</span>

    <span class="n">assembler</span> <span class="o">=</span> <span class="n">mlf</span><span class="o">.</span><span class="n">VectorAssembler</span><span class="p">(</span><span class="n">inputCols</span><span class="o">=</span><span class="n">binned_cols</span><span class="p">,</span> <span class="n">outputCol</span><span class="o">=</span><span class="n">features_col</span><span class="p">)</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">assembler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">features_col</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">df</span><span class="p">,</span> <span class="n">binned_cols</span></div>


<div class="viewcode-block" id="reduce_dimensionality"><a class="viewcode-back" href="../../propensity_matching.html#propensity_matching.utils.reduce_dimensionality">[docs]</a><span class="k">def</span> <span class="nf">reduce_dimensionality</span><span class="p">(</span><span class="n">args</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;chi&#39;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;entry point for dimensionality reduction functions.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    args : dict</span>
<span class="sd">        kwargs dicts w/ arguments for chi or log dim reduction method</span>
<span class="sd">    method : {&#39;chi&#39;, log&#39;}</span>
<span class="sd">        which dim reduction method to use</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    reduce_chi_dimensionality</span>
<span class="sd">    reduce_log_dimensionality</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">func_dict</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;chi&#39;</span><span class="p">:</span> <span class="n">reduce_chi_dimensionality</span><span class="p">,</span>
        <span class="s1">&#39;log&#39;</span><span class="p">:</span> <span class="n">reduce_log_dimensionality</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">func_dict</span><span class="p">[</span><span class="n">method</span><span class="p">](</span><span class="o">**</span><span class="n">args</span><span class="p">)</span></div>


<div class="viewcode-block" id="reduce_chi_dimensionality"><a class="viewcode-back" href="../../propensity_matching.html#propensity_matching.utils.reduce_chi_dimensionality">[docs]</a><span class="k">def</span> <span class="nf">reduce_chi_dimensionality</span><span class="p">(</span>
        <span class="n">df</span><span class="p">:</span> <span class="n">DataFrame</span><span class="p">,</span>
        <span class="n">label_col</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">binned_features_col</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">ncols</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">drop_uninformative</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">sample_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">**</span> <span class="mi">4</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot; Use chi-squared to pick the most informative features</span>

<span class="sd">    Find the most informative features using chi squared. Input features</span>
<span class="sd">    should be categorical. Uninformative features may dropped using fwe.</span>
<span class="sd">    Under-the-hood multiple chi squared tests are run w/ `sample_size`</span>
<span class="sd">    to account for data limitations</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    df : pyspark.sql.DataFrame</span>
<span class="sd">    label_col : str</span>
<span class="sd">        colname of label column. should be 1 or 0 with smaller treatment</span>
<span class="sd">        class as 1</span>
<span class="sd">    binned_features_col : str</span>
<span class="sd">        column of predictors assembled into a vector</span>
<span class="sd">    ncols : int, Optional</span>
<span class="sd">        number of desired output columns. If unspecified, will default to</span>
<span class="sd">        the number of positive samples/`SAMPLES_PER_FEATURE`</span>
<span class="sd">    drop_uninformative : bool, Optional</span>
<span class="sd">        defaults to True. uses fwe w/ significance of .05 to drop</span>
<span class="sd">        uninformative columns</span>
<span class="sd">    sample_size : int, Optional</span>
<span class="sd">        sample size used in each run to evaluate significance of each</span>
<span class="sd">        predictor. defaults to 10**4. This threshold was chosen empirically</span>
<span class="sd">        can be passed as None to avoid sampling ( not recommended)</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    df : pyspark.sql.DataFrame</span>
<span class="sd">        input dataframe, but `binned_features_col` now consists of selected</span>
<span class="sd">        columns.</span>
<span class="sd">    selected_cols : List[str]</span>
<span class="sd">        list of chosen columns</span>

<span class="sd">    Raises</span>
<span class="sd">    ------</span>
<span class="sd">    AssertionError</span>
<span class="sd">        if `ncols` is less than 1, either through calculation or</span>
<span class="sd">        specification</span>
<span class="sd">        if no columns are informative</span>
<span class="sd">    UncaughtExceptions</span>
<span class="sd">        non-int values for ncols</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    using ChiSqSelector with many samples will make p-values of</span>
<span class="sd">    columns in internal calculations identical. Sorting by p-value and</span>
<span class="sd">    returning therefore relies on input order. This method</span>
<span class="sd">    circumvents that by subsampling to `sample_size` and running</span>
<span class="sd">    int(math.ceil(math.log(all_count/sample_size))) times</span>
<span class="sd">    (or 1 when df size is less than sample_size). Each iteration,</span>
<span class="sd">    the rank for each column is calculated and added to previous ranks.</span>
<span class="sd">    The top ncol columns are chosen with the smallest aggregate rank.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_persist_if_unpersisted</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
    <span class="n">binned_pred_cols</span> <span class="o">=</span> <span class="n">_get_pred_cols</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">binned_features_col</span><span class="p">)</span>

    <span class="c1"># if ncols is not specified, set it based on samples specified per feature and positive sample count</span>
    <span class="k">if</span> <span class="n">ncols</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">pos_count</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="n">label_col</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>
        <span class="n">ncols</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">pos_count</span> <span class="o">//</span> <span class="n">SAMPLES_PER_FEATURE</span><span class="p">)</span>

    <span class="k">assert</span> <span class="n">ncols</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;cannot return less than 1 column&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ncols</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;ncols is not int but type </span><span class="si">{type_}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">type_</span><span class="o">=</span><span class="nb">type</span><span class="p">(</span><span class="n">ncols</span><span class="p">)))</span>

    <span class="k">if</span> <span class="n">ncols</span> <span class="o">&lt;=</span> <span class="nb">len</span><span class="p">(</span><span class="n">binned_pred_cols</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">drop_uninformative</span><span class="p">:</span>
            <span class="n">informative_selector</span> <span class="o">=</span> <span class="n">mlf</span><span class="o">.</span><span class="n">ChiSqSelector</span><span class="p">(</span><span class="n">selectorType</span><span class="o">=</span><span class="s1">&#39;fwe&#39;</span><span class="p">,</span>
                                                     <span class="n">fwe</span><span class="o">=.</span><span class="mi">05</span><span class="p">,</span>
                                                     <span class="n">labelCol</span><span class="o">=</span><span class="n">label_col</span><span class="p">,</span>
                                                     <span class="n">featuresCol</span><span class="o">=</span><span class="n">binned_features_col</span><span class="p">)</span>
            <span class="n">selected_cols</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span>
                             <span class="nb">list</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">binned_pred_cols</span><span class="p">)[</span><span class="n">informative_selector</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">df</span><span class="p">)</span><span class="o">.</span><span class="n">selectedFeatures</span><span class="p">])]</span>
            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">selected_cols</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;no informative columns found&quot;</span>
            <span class="n">assembler</span> <span class="o">=</span> <span class="n">mlf</span><span class="o">.</span><span class="n">VectorAssembler</span><span class="p">(</span><span class="n">inputCols</span><span class="o">=</span><span class="n">selected_cols</span><span class="p">,</span> <span class="n">outputCol</span><span class="o">=</span><span class="n">binned_features_col</span><span class="p">)</span>
            <span class="n">df</span> <span class="o">=</span> <span class="n">assembler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">binned_features_col</span><span class="p">))</span>
            <span class="k">return</span> <span class="n">df</span><span class="p">,</span> <span class="n">selected_cols</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">df</span><span class="p">,</span> <span class="n">binned_pred_cols</span>

    <span class="n">all_count</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>

    <span class="k">if</span> <span class="p">(</span><span class="n">sample_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span> <span class="o">|</span> <span class="p">(</span><span class="n">all_count</span> <span class="o">&lt;=</span> <span class="n">sample_size</span><span class="p">):</span>
        <span class="n">num_runs</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">sample_frac</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>  <span class="c1"># all_count &gt; sample_size:</span>
        <span class="n">num_runs</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">all_count</span> <span class="o">/</span> <span class="n">sample_size</span><span class="p">)))</span>
        <span class="n">sample_frac</span> <span class="o">=</span> <span class="n">sample_size</span> <span class="o">/</span> <span class="n">all_count</span>

    <span class="n">col_overall_ranks</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">feature_ranker</span> <span class="o">=</span> <span class="n">mlf</span><span class="o">.</span><span class="n">ChiSqSelector</span><span class="p">(</span><span class="n">selectorType</span><span class="o">=</span><span class="s1">&#39;numTopFeatures&#39;</span><span class="p">,</span>
                                       <span class="n">numTopFeatures</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">binned_pred_cols</span><span class="p">),</span>
                                       <span class="n">labelCol</span><span class="o">=</span><span class="n">label_col</span><span class="p">,</span>
                                       <span class="n">featuresCol</span><span class="o">=</span><span class="n">binned_features_col</span><span class="p">)</span>

    <span class="c1"># test binning</span>
    <span class="k">try</span><span class="p">:</span>

        <span class="n">start_run</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">except</span> <span class="p">:</span>
        <span class="n">start_run</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="c1"># for each run, add significance rank of column to previous ranks.</span>
    <span class="k">for</span> <span class="n">run</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">start_run</span><span class="p">,</span> <span class="n">num_runs</span><span class="p">):</span>
        <span class="c1"># while we usually specify the seed, we rely on randomness of</span>
        <span class="c1"># the seeds here. perhaps introduce a prng where we specify</span>
        <span class="c1"># the seed, fulfilling reproducibility and randomness needs</span>
        <span class="n">sampled_df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">withReplacement</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">fraction</span><span class="o">=</span><span class="n">sample_frac</span><span class="p">)</span>
        <span class="n">sampled_df</span><span class="o">.</span><span class="n">persist</span><span class="p">(</span><span class="n">StorageLevel</span><span class="p">(</span><span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">))</span>

        <span class="n">cols</span> <span class="o">=</span> <span class="n">feature_ranker</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">sampled_df</span><span class="p">)</span><span class="o">.</span><span class="n">selectedFeatures</span>
        <span class="n">col_ranks</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="n">cols</span><span class="p">,</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">cols</span><span class="p">))))</span>
        <span class="k">for</span> <span class="n">col</span><span class="p">,</span> <span class="n">rank</span> <span class="ow">in</span> <span class="n">col_ranks</span><span class="p">:</span>
            <span class="n">col_overall_ranks</span><span class="p">[</span><span class="n">col</span><span class="p">]</span> <span class="o">+=</span> <span class="n">rank</span>

    <span class="c1"># pick top ncol cols with lowest aggregate ranks</span>
    <span class="n">col_overall_ranks</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">col_overall_ranks</span><span class="o">.</span><span class="n">items</span><span class="p">())</span>
    <span class="n">col_sorted_ranks</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">col_overall_ranks</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">selected_col_indices</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">col_sorted_ranks</span><span class="p">[:</span><span class="n">ncols</span><span class="p">]]</span>
    <span class="n">selected_cols</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">binned_pred_cols</span><span class="p">)[</span><span class="n">selected_col_indices</span><span class="p">])]</span>

    <span class="n">assembler</span> <span class="o">=</span> <span class="n">mlf</span><span class="o">.</span><span class="n">VectorAssembler</span><span class="p">(</span><span class="n">inputCols</span><span class="o">=</span><span class="n">selected_cols</span><span class="p">,</span> <span class="n">outputCol</span><span class="o">=</span><span class="n">binned_features_col</span><span class="p">)</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">assembler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">binned_features_col</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">drop_uninformative</span><span class="p">:</span>
        <span class="n">informative_selector</span> <span class="o">=</span> <span class="n">mlf</span><span class="o">.</span><span class="n">ChiSqSelector</span><span class="p">(</span><span class="n">selectorType</span><span class="o">=</span><span class="s1">&#39;fwe&#39;</span><span class="p">,</span>
                                                 <span class="n">fwe</span><span class="o">=.</span><span class="mi">05</span><span class="p">,</span>
                                                 <span class="n">labelCol</span><span class="o">=</span><span class="n">label_col</span><span class="p">,</span>
                                                 <span class="n">featuresCol</span><span class="o">=</span><span class="n">binned_features_col</span><span class="p">)</span>
        <span class="n">selected_cols</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">selected_cols</span><span class="p">)[</span><span class="n">informative_selector</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">df</span><span class="p">)</span><span class="o">.</span><span class="n">selectedFeatures</span><span class="p">])]</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">selected_cols</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;no informative columns found&quot;</span>
        <span class="n">assembler</span> <span class="o">=</span> <span class="n">mlf</span><span class="o">.</span><span class="n">VectorAssembler</span><span class="p">(</span><span class="n">inputCols</span><span class="o">=</span><span class="n">selected_cols</span><span class="p">,</span> <span class="n">outputCol</span><span class="o">=</span><span class="n">binned_features_col</span><span class="p">)</span>
        <span class="n">df</span> <span class="o">=</span> <span class="n">assembler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">binned_features_col</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">df</span><span class="p">,</span> <span class="n">selected_cols</span></div>


<div class="viewcode-block" id="reduce_log_dimensionality"><a class="viewcode-back" href="../../propensity_matching.html#propensity_matching.utils.reduce_log_dimensionality">[docs]</a><span class="k">def</span> <span class="nf">reduce_log_dimensionality</span><span class="p">(</span><span class="n">df</span><span class="p">:</span> <span class="n">DataFrame</span><span class="p">,</span>
                              <span class="n">label_col</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
                              <span class="n">binned_features_col</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
                              <span class="n">ncols</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                              <span class="n">sample_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">**</span> <span class="mi">6</span><span class="p">,</span>
                              <span class="n">log_args</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;use logistic regression on normalized predictors to find top</span>
<span class="sd">    ncols with greatest absolute beta coefficients</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    df : pyspark.sql.DataFrame</span>
<span class="sd">    binned_pred_cols : List[str]</span>
<span class="sd">        list of normalized predictors assembled in `binned_features_col`</span>
<span class="sd">    label_col : str</span>
<span class="sd">        colname with class labels. Should be binary, with 1 being the</span>
<span class="sd">        smaller treatment class</span>
<span class="sd">    binned_features_col : str</span>
<span class="sd">        colname of categorical predictors assembled into a vector</span>
<span class="sd">    ncols : int, optional</span>
<span class="sd">        defaults to positive_samples/`SAMPLES_PER_FEATURE`</span>
<span class="sd">        number of columns to return</span>
<span class="sd">    sample_size : int, optional</span>
<span class="sd">        defaults to 10**6, size of sample to train logistic regression on</span>
<span class="sd">        can be passed as None to avoid sampling</span>
<span class="sd">    log_args : dict, optional</span>
<span class="sd">        kwargs dict for pyspark.ml.classification.LogisticRegression</span>
<span class="sd">        estimator initialization. Otherwise specifies label, feature col</span>
<span class="sd">        and uses pyspark defaults. must include featureCol and labelCol</span>
<span class="sd">        arguments. label_col, binned_features_col args will be ignored</span>
<span class="sd">        but must still be passed</span>


<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    df : pyspark.sql.DataFrame</span>
<span class="sd">        input df but `binned_features_cols` is now a vector of chosen columns</span>
<span class="sd">    selected_cols : List[str]</span>
<span class="sd">        list of chosen columns</span>

<span class="sd">    Raises</span>
<span class="sd">    ------</span>
<span class="sd">    AssertionError</span>
<span class="sd">        ncol less than 1 by calculation or specification</span>
<span class="sd">    UncaughtExceptions</span>
<span class="sd">        non-int vals for ncol</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    Assumes input variables have been normalized. Train log regression and</span>
<span class="sd">    picks `ncols` with greatest absolute coefficient. Does not guarantee</span>
<span class="sd">    this combination of columns will have greatest predictive power,</span>
<span class="sd">    especially in cases with high-impact low-presence predictors.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_persist_if_unpersisted</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
    <span class="n">binned_pred_cols</span> <span class="o">=</span> <span class="n">_get_pred_cols</span><span class="p">(</span><span class="n">df</span><span class="o">=</span><span class="n">df</span><span class="p">,</span> <span class="n">features_col</span><span class="o">=</span><span class="n">binned_features_col</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">ncols</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">pos_count</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">col</span><span class="p">(</span><span class="n">label_col</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>
        <span class="n">ncols</span> <span class="o">=</span> <span class="n">pos_count</span> <span class="o">//</span> <span class="n">SAMPLES_PER_FEATURE</span>

    <span class="k">assert</span> <span class="n">ncols</span> <span class="o">&gt;=</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;cannot return less than 1 column&quot;</span>

    <span class="k">if</span> <span class="n">log_args</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">log_estimator</span> <span class="o">=</span> <span class="n">mlc</span><span class="o">.</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">featuresCol</span><span class="o">=</span><span class="n">binned_features_col</span><span class="p">,</span> <span class="n">labelCol</span><span class="o">=</span><span class="n">label_col</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">log_estimator</span> <span class="o">=</span> <span class="n">mlc</span><span class="o">.</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="o">**</span><span class="n">log_args</span><span class="p">)</span>

    <span class="n">all_count</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">sample_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span> <span class="o">|</span> <span class="p">(</span><span class="n">all_count</span> <span class="o">&lt;=</span> <span class="n">sample_size</span><span class="p">):</span>
        <span class="n">sample_df</span> <span class="o">=</span> <span class="n">df</span>
    <span class="k">else</span><span class="p">:</span>  <span class="c1"># all_count &gt; sample_size:</span>
        <span class="n">frac</span> <span class="o">=</span> <span class="n">sample_size</span> <span class="o">/</span> <span class="n">all_count</span>
        <span class="n">sample_df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">withReplacement</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">fraction</span><span class="o">=</span> <span class="n">frac</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
        <span class="n">sample_df</span><span class="o">.</span><span class="n">persist</span><span class="p">(</span><span class="n">StorageLevel</span><span class="p">(</span><span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

    <span class="n">log_model</span> <span class="o">=</span> <span class="n">log_estimator</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">sample_df</span><span class="p">)</span>
    <span class="c1"># sort abs value greatest to least</span>
    <span class="n">coeffs</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">binned_pred_cols</span><span class="p">,</span> <span class="n">log_model</span><span class="o">.</span><span class="n">coefficients</span><span class="p">),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="o">-</span><span class="nb">abs</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
    <span class="n">selected_cols</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">coeffs</span><span class="p">[:</span><span class="n">ncols</span><span class="p">]]</span>

    <span class="n">assembler</span> <span class="o">=</span> <span class="n">mlf</span><span class="o">.</span><span class="n">VectorAssembler</span><span class="p">(</span><span class="n">inputCols</span><span class="o">=</span><span class="n">selected_cols</span><span class="p">,</span> <span class="n">outputCol</span><span class="o">=</span><span class="n">binned_features_col</span><span class="p">)</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">assembler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">binned_features_col</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">df</span><span class="p">,</span> <span class="n">selected_cols</span></div>
</pre></div>

          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../propensity_matching_docs.html">propensity_matching  documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../index.html" >Module code</a> &#187;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2018, ruaghaya.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.7.4.
    </div>
  </body>
</html>